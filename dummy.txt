    # for d in dataset:
    #     #print(type(d))
    #     print(d[0]) # caption tokens
    #     print(' '.join(GPT2Tokenizer.from_pretrained('gpt2').convert_ids_to_tokens(d[0])))
    #     print()
    #     print(d[1])  #maska
    #     print()
    #     print(d[2])
    #     print(d[2].size()) # 1 x 512  # clip embeddings
    #     print()
    #     break


    with open(args.data, 'rb') as f:
        all_data = pickle.load(f)

    print()
    counter = 0
    for a in all_data['captions']:
        print(a['answer'])
        counter = counter + 1
        if counter == 100:
            break


    with torch.no_grad():
        for d in tqdm(data_loader):
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["labels"].to(device)
            outputs = model(input_ids=input_ids,attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)
            loss = loss_fn(outputs, targets)
            correct_predictions += torch.sum(preds == targets)
            losses.append(loss.item())
        return correct_predictions.double() / n_examples, np.mean(losses)


# EXAMPLE
# {'image_id': 131557, 'question': 'What are the sources of protein on this pizza?', 'answer': 'cheese', 'caption': 'A close up of a pizza pie on a plate.', 'clip_embedding': 1024}
# {'image_id': 260409, 'id': 24152, 'caption': 'A person posing next to their motorcycle by the shore.', 'clip_embedding': 1021}


    #train_dataloader = DataLoader(dataset, batch_size=1, drop_last=True)
    #for idx, d in enumerate(train_dataloader):
        #temp = idx
        #temp_tokens, temp_mask, temp_prefix = d



        with open(data_path, 'rb') as f:
            all_data = pickle.load(f)

{'image_id': 131557,
'question': 'What are the sources of protein on this pizza?',
 'answer': 'cheese',
 'caption': 'A close up of a pizza pie on a plate.',
 'clip_embedding': 1024}



    with open(args.data, 'rb') as f:
        all_data = pickle.load(f)
    print(all_data)
    ans = []
    for a in all_data.get('captions'):
        ans.append(a.get('answer').split())
    print(ans)
    print()
    ans.sort(key=lambda x: len(x))
    print(ans)



            ####################################################
            # modified_mask = mask[:, dataset.prefix_length : -1]

            # modified_mask_v2 = mask.squeeze()[ 10: ]
            # bool_mask = modified_mask_v2.ge(1).unsqueeze(1)
            # tempy  = logits.squeeze()
            # tryy = tempy[bool_mask]
            # valid_mask = need_predict == 1
            # #valid_mask2 = target != self.padding_idx
            # #assert (valid_mask.long() - valid_mask2.long()).abs().sum().cpu() == 0
            # target = target[valid_mask]
            # feat = feat[valid_mask]
            # apo 1 -21 -50257 se 21 -50257 (from head model) tryy
            # loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokenized_answer.flatten(), ignore_index=0)
            ####################################################
    # for d in dataset:
    #     #print(type(d))
    #     print(d[0]) # caption tokens
    #     print(' '.join(GPT2Tokenizer.from_pretrained('gpt2').convert_ids_to_tokens(d[0])))
    #     print()
    #     print(d[1])  #maska
    #     print()
    #     print(d[2])
    #     print(d[2].size()) # 1 x 512  # clip embeddings
    #     print()
    #     break


    #train_dataloader = DataLoader(dataset, batch_size=1, drop_last=True)
    #for idx, d in enumerate(train_dataloader):
        #temp = idx
        #temp_tokens, temp_mask, temp_prefix = d



        with open(data_path, 'rb') as f:
            all_data = pickle.load(f)

{'image_id': 131557,
'question': 'What are the sources of protein on this pizza?',
 'answer': 'cheese',
 'caption': 'A close up of a pizza pie on a plate.',
 'clip_embedding': 1024}



    with open(args.data, 'rb') as f:
        all_data = pickle.load(f)
    print(all_data)
    ans = []
    for a in all_data.get('captions'):
        ans.append(a.get('answer').split())
    print(ans)
    print()
    ans.sort(key=lambda x: len(x))
    print(ans)



            ####################################################
            # modified_mask = mask[:, dataset.prefix_length : -1]

            # modified_mask_v2 = mask.squeeze()[ 10: ]
            # bool_mask = modified_mask_v2.ge(1).unsqueeze(1)
            # tempy  = logits.squeeze()
            # tryy = tempy[bool_mask]
            # valid_mask = need_predict == 1
            # #valid_mask2 = target != self.padding_idx
            # #assert (valid_mask.long() - valid_mask2.long()).abs().sum().cpu() == 0
            # target = target[valid_mask]
            # feat = feat[valid_mask]
            # apo 1 -21 -50257 se 21 -50257 (from head model) tryy
            # loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokenized_answer.flatten(), ignore_index=0)
            ####################################################